{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51086367",
   "metadata": {},
   "source": [
    "## Generate the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be311771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "sys.path.append('..') # Add '#' at the beginning of this line if the pipeline does not work on your computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88d459",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f6d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: f:\\Upenn\\CIS5450 - big data analysis\\project_repo\\CIS545_Repo\\config\\parameters.yaml\n",
      "Loading data...\n",
      "Loading price data from: f:\\Upenn\\CIS5450 - big data analysis\\project_repo\\CIS545_Repo\\data/raw/AAPL.US_M5.xlsx\n",
      "Price data shape: (104028, 6)\n",
      "Loading news data from: f:\\Upenn\\CIS5450 - big data analysis\\project_repo\\CIS545_Repo\\data/raw/apple_news_data.xlsx\n",
      "News data shape: (42127, 27)\n",
      "Loading fundamental data from: f:\\Upenn\\CIS5450 - big data analysis\\project_repo\\CIS545_Repo\\data/raw/iw0erafaqkjcbrw6.xlsx\n",
      "Fundamental data shape: (76, 78)\n",
      "SUCCESS: All data loaded!\n",
      "Price data: (104028, 6)\n",
      "News data: (42127, 27)\n",
      "Fundamental data: (76, 78)\n",
      "Price data columns: ['datetime', 'open', 'high', 'low', 'close'] ...\n",
      "News data columns: ['date', 'title', 'content', 'link', 'symbols'] ...\n",
      "Fundamental data columns: ['gvkey', 'permno', 'adate', 'qdate', 'public_date'] ...\n",
      "Fundamental data sample:\n",
      "   gvkey  permno      adate      qdate public_date   CAPEI     bm     evm  \\\n",
      "0   1690   14593 2017-09-30 2017-09-30  2018-01-31  18.965  0.210  12.803   \n",
      "1   1690   14593 2017-09-30 2017-12-31  2018-02-28  20.128  0.164  12.091   \n",
      "2   1690   14593 2017-09-30 2017-12-31  2018-03-31  18.471  0.164  12.091   \n",
      "\n",
      "   pe_op_basic  pe_op_dil  ...  adv_sale  staff_sale  accrual    ptb  \\\n",
      "0       18.061     18.179  ...         0           0   -0.044  5.132   \n",
      "1        4.323      4.353  ...         0           0   -0.040  6.421   \n",
      "2        4.072      4.100  ...         0           0   -0.040  5.893   \n",
      "\n",
      "   PEG_trailing  divyield  PEG_1yrforward  PEG_ltgforward  TICKER    cusip  \n",
      "0         1.242    0.0151           0.738           1.629    AAPL  3783310  \n",
      "1         0.427    0.0141           0.179           0.392    AAPL  3783310  \n",
      "2         0.403    0.0150           0.175           0.388    AAPL  3783310  \n",
      "\n",
      "[3 rows x 78 columns]\n"
     ]
    }
   ],
   "source": [
    "from src.data_loader import DataLoader\n",
    "\n",
    "# Load all data\n",
    "loader = DataLoader()\n",
    "data = loader.load_data()\n",
    "\n",
    "price_data = data['price_data']\n",
    "news_data = data['news_data']\n",
    "fundamental_data = data['fundamental_data']\n",
    "\n",
    "print(\"SUCCESS: All data loaded!\")\n",
    "print(f\"Price data: {price_data.shape}\")\n",
    "print(f\"News data: {news_data.shape}\")\n",
    "print(f\"Fundamental data: {fundamental_data.shape}\")\n",
    "\n",
    "print(\"Price data columns:\", price_data.columns.tolist()[:5], \"...\")\n",
    "print(\"News data columns:\", news_data.columns.tolist()[:5], \"...\")\n",
    "print(\"Fundamental data columns:\", fundamental_data.columns.tolist()[:5], \"...\")\n",
    "\n",
    "print(\"Fundamental data sample:\")\n",
    "print(fundamental_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CHECKING DATA STRUCTURE\n",
      "\n",
      "==================================================\n",
      "üìä PRICE DATA STRUCTURE\n",
      "==================================================\n",
      "Shape: (104028, 6)\n",
      "Columns: ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
      "Data types:\n",
      "datetime    datetime64[ns]\n",
      "open               float64\n",
      "high               float64\n",
      "low                float64\n",
      "close              float64\n",
      "volume               int64\n",
      "dtype: object\n",
      "\n",
      "First 2 rows:\n",
      "             datetime   open   high    low  close  volume\n",
      "0 2018-11-30 17:50:00  44.60  44.64  44.55  44.63  179494\n",
      "1 2018-11-30 17:55:00  44.63  44.63  44.52  44.52  218881\n",
      "\n",
      "==================================================\n",
      "üì∞ NEWS DATA STRUCTURE\n",
      "==================================================\n",
      "Shape: (42127, 27)\n",
      "Columns: ['date', 'title', 'content', 'link', 'symbols', 'tags', 'sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26']\n",
      "Data types:\n",
      "date                   object\n",
      "title                  object\n",
      "content                object\n",
      "link                   object\n",
      "symbols                object\n",
      "tags                   object\n",
      "sentiment_polarity     object\n",
      "sentiment_neg          object\n",
      "sentiment_neu          object\n",
      "sentiment_pos          object\n",
      "Unnamed: 10            object\n",
      "Unnamed: 11            object\n",
      "Unnamed: 12            object\n",
      "Unnamed: 13            object\n",
      "Unnamed: 14            object\n",
      "Unnamed: 15            object\n",
      "Unnamed: 16            object\n",
      "Unnamed: 17            object\n",
      "Unnamed: 18            object\n",
      "Unnamed: 19            object\n",
      "Unnamed: 20            object\n",
      "Unnamed: 21            object\n",
      "Unnamed: 22            object\n",
      "Unnamed: 23            object\n",
      "Unnamed: 24            object\n",
      "Unnamed: 25            object\n",
      "Unnamed: 26           float64\n",
      "dtype: object\n",
      "\n",
      "First 2 rows:\n",
      "                        date  \\\n",
      "0  2024-11-27T16:39:00+00:00   \n",
      "1  2024-11-26T00:00:00+00:00   \n",
      "\n",
      "                                               title  \\\n",
      "0  Berkshire Stock Hits Record Even as Company Re...   \n",
      "1                      What Is a Stock Market Index?   \n",
      "\n",
      "                                             content  \\\n",
      "0  Warren BuffettÈà•Ê™ö caution, his advancing age, a...   \n",
      "1                      What Is a Stock Market Index?   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://finance.yahoo.com/m/f5df3aa4-364b-31d6...   \n",
      "1  https://www.fool.com/investing/stock-market/in...   \n",
      "\n",
      "                                             symbols tags sentiment_polarity  \\\n",
      "0  0R2V.IL, AAPL.BA, AAPL.MX, AAPL.NEO, AAPL.SN, ...  NaN                  0   \n",
      "1                          AAPL.US, AMZN.US, MSFT.US  NaN                  0   \n",
      "\n",
      "  sentiment_neg sentiment_neu sentiment_pos  ... Unnamed: 17 Unnamed: 18  \\\n",
      "0             0             1             0  ...         NaN         NaN   \n",
      "1             0             1             0  ...         NaN         NaN   \n",
      "\n",
      "  Unnamed: 19 Unnamed: 20 Unnamed: 21 Unnamed: 22 Unnamed: 23 Unnamed: 24  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 25 Unnamed: 26  \n",
      "0         NaN         NaN  \n",
      "1         NaN         NaN  \n",
      "\n",
      "[2 rows x 27 columns]\n",
      "\n",
      "==================================================\n",
      "üí∞ FUNDAMENTAL DATA STRUCTURE\n",
      "==================================================\n",
      "Shape: (76, 78)\n",
      "Columns: ['gvkey', 'permno', 'adate', 'qdate', 'public_date', 'CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', 'pe_exi', 'pe_inc', 'ps', 'pcf', 'dpr', 'npm', 'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe', 'roce', 'efftax', 'aftret_eq', 'aftret_invcapx', 'aftret_equity', 'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap', 'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt', 'int_totdebt', 'cash_lt', 'invt_act', 'rect_act', 'debt_at', 'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt', 'profit_lct', 'ocf_lct', 'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets', 'debt_capital', 'de_ratio', 'intcov', 'intcov_ratio', 'cash_ratio', 'quick_ratio', 'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn', 'rect_turn', 'pay_turn', 'sale_invcap', 'sale_equity', 'sale_nwc', 'rd_sale', 'adv_sale', 'staff_sale', 'accrual', 'ptb', 'PEG_trailing', 'divyield', 'PEG_1yrforward', 'PEG_ltgforward', 'TICKER', 'cusip']\n",
      "Data types:\n",
      "gvkey                      int64\n",
      "permno                     int64\n",
      "adate             datetime64[ns]\n",
      "qdate             datetime64[ns]\n",
      "public_date       datetime64[ns]\n",
      "                       ...      \n",
      "divyield                 float64\n",
      "PEG_1yrforward           float64\n",
      "PEG_ltgforward           float64\n",
      "TICKER                    object\n",
      "cusip                      int64\n",
      "Length: 78, dtype: object\n",
      "\n",
      "First 2 rows:\n",
      "   gvkey  permno      adate      qdate public_date   CAPEI     bm     evm  \\\n",
      "0   1690   14593 2017-09-30 2017-09-30  2018-01-31  18.965  0.210  12.803   \n",
      "1   1690   14593 2017-09-30 2017-12-31  2018-02-28  20.128  0.164  12.091   \n",
      "\n",
      "   pe_op_basic  pe_op_dil  ...  adv_sale  staff_sale  accrual    ptb  \\\n",
      "0       18.061     18.179  ...         0           0   -0.044  5.132   \n",
      "1        4.323      4.353  ...         0           0   -0.040  6.421   \n",
      "\n",
      "   PEG_trailing  divyield  PEG_1yrforward  PEG_ltgforward  TICKER    cusip  \n",
      "0         1.242    0.0151           0.738           1.629    AAPL  3783310  \n",
      "1         0.427    0.0141           0.179           0.392    AAPL  3783310  \n",
      "\n",
      "[2 rows x 78 columns]\n",
      "\n",
      "==================================================\n",
      "üîç SENTIMENT COLUMNS IN NEWS DATA\n",
      "==================================================\n",
      "Sentiment-related columns: ['sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos']\n",
      "  sentiment_polarity: object\n",
      "  Sample values: [0, 0, 0]\n",
      "  sentiment_neg: object\n",
      "  Sample values: [0, 0, 0]\n",
      "  sentiment_neu: object\n",
      "  Sample values: [1, 1, 1]\n",
      "  sentiment_pos: object\n",
      "  Sample values: [0, 0, 0]\n",
      "\n",
      "==================================================\n",
      "üìÖ DATE COLUMNS IN EACH DATASET\n",
      "==================================================\n",
      "Price data date columns: ['datetime']\n",
      "News data date columns: ['date', 'sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos']\n",
      "Fundamental data date columns: ['adate', 'qdate', 'public_date']\n"
     ]
    }
   ],
   "source": [
    "# Check data structure before merging\n",
    "print(\" CHECKING DATA STRUCTURE\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" PRICE DATA STRUCTURE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {price_data.shape}\")\n",
    "print(f\"Columns: {price_data.columns.tolist()}\")\n",
    "print(f\"Data types:\")\n",
    "print(price_data.dtypes)\n",
    "print(f\"\\nFirst 2 rows:\")\n",
    "print(price_data.head(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" NEWS DATA STRUCTURE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {news_data.shape}\")\n",
    "print(f\"Columns: {news_data.columns.tolist()}\")\n",
    "print(f\"Data types:\")\n",
    "print(news_data.dtypes)\n",
    "print(f\"\\nFirst 2 rows:\")\n",
    "print(news_data.head(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" FUNDAMENTAL DATA STRUCTURE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {fundamental_data.shape}\")\n",
    "print(f\"Columns: {fundamental_data.columns.tolist()}\")\n",
    "print(f\"Data types:\")\n",
    "print(fundamental_data.dtypes)\n",
    "print(f\"\\nFirst 2 rows:\")\n",
    "print(fundamental_data.head(2))\n",
    "\n",
    "# Check for sentiment-related columns in news data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" SENTIMENT COLUMNS IN NEWS DATA\")\n",
    "print(\"=\"*50)\n",
    "sentiment_cols = [col for col in news_data.columns if 'sentiment' in col.lower()]\n",
    "print(f\"Sentiment-related columns: {sentiment_cols}\")\n",
    "if sentiment_cols:\n",
    "    for col in sentiment_cols:\n",
    "        print(f\"  {col}: {news_data[col].dtype}\")\n",
    "        print(f\"  Sample values: {news_data[col].head(3).tolist()}\")\n",
    "\n",
    "# Check date columns in each dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" DATE COLUMNS IN EACH DATASET1\")\n",
    "print(\"=\"*50)\n",
    "print(\"Price data date columns:\", [col for col in price_data.columns if 'date' in col.lower() or 'time' in col.lower()])\n",
    "print(\"News data date columns:\", [col for col in news_data.columns if 'date' in col.lower() or 'time' in col.lower()])\n",
    "print(\"Fundamental data date columns:\", [col for col in fundamental_data.columns if 'date' in col.lower() or 'time' in col.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee397f21",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da105a",
   "metadata": {},
   "source": [
    "### 1: Check data structure in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d017a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price Data:\n",
      "  Shape: (104028, 7)\n",
      "  Columns: ['datetime', 'open', 'high', 'low', 'close', 'volume', 'merge_date']\n",
      "  First datetime: 2018-11-30 17:50:00\n",
      "  Last datetime: 2024-03-13 23:00:00\n",
      "\n",
      "News Data:\n",
      "  Shape: (29751, 28)\n",
      "  Date columns: ['date', 'merge_date']\n",
      "  Sentiment columns: ['sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos']\n",
      "\n",
      "Fundamental Data:\n",
      "  Shape: (76, 79)\n",
      "  Date columns: ['adate', 'qdate', 'public_date', 'merge_date']\n",
      "  Key metrics: ['CAPEI', 'bm', 'evm', 'pe_op_basic', 'roa', 'roe']\n"
     ]
    }
   ],
   "source": [
    "print(\"Price Data:\")\n",
    "print(f\"  Shape: {price_data.shape}\")\n",
    "print(f\"  Columns: {price_data.columns.tolist()}\")\n",
    "print(f\"  First datetime: {price_data['datetime'].min()}\")\n",
    "print(f\"  Last datetime: {price_data['datetime'].max()}\")\n",
    "\n",
    "print(\"\\nNews Data:\")\n",
    "print(f\"  Shape: {news_data.shape}\")\n",
    "print(f\"  Date columns: {[col for col in news_data.columns if 'date' in col.lower()]}\")\n",
    "print(f\"  Sentiment columns: {[col for col in news_data.columns if 'sentiment' in col.lower()]}\")\n",
    "\n",
    "print(\"\\nFundamental Data:\")\n",
    "print(f\"  Shape: {fundamental_data.shape}\")\n",
    "print(f\"  Date columns: {[col for col in fundamental_data.columns if 'date' in col.lower()]}\")\n",
    "print(f\"  Key metrics: {['CAPEI', 'bm', 'evm', 'pe_op_basic', 'roa', 'roe']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4be5fc",
   "metadata": {},
   "source": [
    "### 2: Create unified date columns for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0935aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Price data: created merge_date from datetime\n",
      "   Date range: 2018-11-30 to 2024-03-13\n",
      " News data: created merge_date from date column\n",
      "   Removed 0 rows with invalid dates\n",
      "   Date range: 2016-02-19 to 2024-11-27\n",
      " Fundamental data: created merge_date from public_date\n",
      "   Date range: 2018-01-31 to 2024-04-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27400\\2405703928.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_data['merge_date'] = pd.to_datetime(news_data['date'], errors='coerce').dt.date\n"
     ]
    }
   ],
   "source": [
    "# Price data: extract date from datetime\n",
    "price_data['merge_date'] = pd.to_datetime(price_data['datetime']).dt.date\n",
    "print(f\" Price data: created merge_date from datetime\")\n",
    "print(f\"   Date range: {price_data['merge_date'].min()} to {price_data['merge_date'].max()}\")\n",
    "\n",
    "# News data: convert and clean date column\n",
    "news_data['merge_date'] = pd.to_datetime(news_data['date'], errors='coerce').dt.date\n",
    "initial_news_count = len(news_data)\n",
    "news_data = news_data.dropna(subset=['merge_date'])\n",
    "cleaned_news_count = len(news_data)\n",
    "print(f\" News data: created merge_date from date column\")\n",
    "print(f\"   Removed {initial_news_count - cleaned_news_count} rows with invalid dates\")\n",
    "print(f\"   Date range: {news_data['merge_date'].min()} to {news_data['merge_date'].max()}\")\n",
    "\n",
    "# Fundamental data: use public_date as reference date\n",
    "fundamental_data['merge_date'] = pd.to_datetime(fundamental_data['public_date']).dt.date\n",
    "print(f\" Fundamental data: created merge_date from public_date\")\n",
    "print(f\"   Date range: {fundamental_data['merge_date'].min()} to {fundamental_data['merge_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01fe6cc",
   "metadata": {},
   "source": [
    "### 3: Process sentiment data and convert to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71de9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted sentiment_polarity: 29462 valid values\n",
      "Converted sentiment_neg: 29474 valid values\n",
      "Converted sentiment_neu: 29496 valid values\n",
      "Converted sentiment_pos: 29476 valid values\n",
      "   Total sentiment columns processed: 4\n"
     ]
    }
   ],
   "source": [
    "sentiment_cols = ['sentiment_polarity', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos']\n",
    "converted_cols = []\n",
    "\n",
    "for col in sentiment_cols:\n",
    "    if col in news_data.columns:\n",
    "        news_data[col] = pd.to_numeric(news_data[col], errors='coerce')\n",
    "        converted_cols.append(col)\n",
    "        valid_count = news_data[col].notna().sum()\n",
    "        print(f\"Converted {col}: {valid_count} valid values\")\n",
    "\n",
    "print(f\"   Total sentiment columns processed: {len(converted_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599e8e1",
   "metadata": {},
   "source": [
    "### 4: Aggregate sentiment data by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b41e022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created daily sentiment aggregation\n",
      "   Unique dates with news: 1574\n",
      "   Total news articles: 29751\n",
      "   Aggregated columns: ['merge_date', 'avg_sentiment_polarity', 'avg_sentiment_neg', 'avg_sentiment_neu', 'avg_sentiment_pos', 'daily_news_count']\n"
     ]
    }
   ],
   "source": [
    "daily_sentiment = news_data.groupby('merge_date').agg({\n",
    "    'sentiment_polarity': 'mean',\n",
    "    'sentiment_neg': 'mean', \n",
    "    'sentiment_neu': 'mean',\n",
    "    'sentiment_pos': 'mean',\n",
    "    'title': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "daily_sentiment.columns = [\n",
    "    'merge_date', \n",
    "    'avg_sentiment_polarity', \n",
    "    'avg_sentiment_neg', \n",
    "    'avg_sentiment_neu', \n",
    "    'avg_sentiment_pos', \n",
    "    'daily_news_count'\n",
    "]\n",
    "\n",
    "print(f\"   Created daily sentiment aggregation\")\n",
    "print(f\"   Unique dates with news: {len(daily_sentiment)}\")\n",
    "print(f\"   Total news articles: {daily_sentiment['daily_news_count'].sum()}\")\n",
    "print(f\"   Aggregated columns: {daily_sentiment.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43d23b",
   "metadata": {},
   "source": [
    "### 5: Merge price data with daily sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d037fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Successfully merged price and sentiment data\n",
      "   Before merge: (104028, 7)\n",
      "   After merge: (104028, 12)\n",
      "   Records with sentiment data: 74220/104028 (71.3%)\n"
     ]
    }
   ],
   "source": [
    "price_with_sentiment = pd.merge(\n",
    "    price_data, \n",
    "    daily_sentiment, \n",
    "    on='merge_date', \n",
    "    how='left'  # Keep all price records\n",
    ")\n",
    "\n",
    "print(f\"   Successfully merged price and sentiment data\")\n",
    "print(f\"   Before merge: {price_data.shape}\")\n",
    "print(f\"   After merge: {price_with_sentiment.shape}\")\n",
    "\n",
    "# Check merge results\n",
    "sentiment_coverage = price_with_sentiment['avg_sentiment_polarity'].notna().sum()\n",
    "total_records = len(price_with_sentiment)\n",
    "print(f\"   Records with sentiment data: {sentiment_coverage}/{total_records} ({sentiment_coverage/total_records*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0c523",
   "metadata": {},
   "source": [
    "### 6: Prepare fundamental data for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STEP 6: PREPARING FUNDAMENTAL DATA\n",
      " Selected fundamental metrics:\n",
      "   - CAPEI: 76 valid values\n",
      "   - bm: 76 valid values\n",
      "   - evm: 76 valid values\n",
      "   - pe_op_basic: 76 valid values\n",
      "   - pe_op_dil: 76 valid values\n",
      "   - roa: 76 valid values\n",
      "   - roe: 76 valid values\n",
      "   - npm: 76 valid values\n",
      "   - ps: 76 valid values\n",
      "   - de_ratio: 76 valid values\n"
     ]
    }
   ],
   "source": [
    "# Select key fundamental metrics\n",
    "key_fundamental_metrics = [\n",
    "    'merge_date', 'CAPEI', 'bm', 'evm', 'pe_op_basic', 'pe_op_dil', \n",
    "    'roa', 'roe', 'npm', 'ps', 'de_ratio'\n",
    "]\n",
    "\n",
    "# Filter to only include columns that exist\n",
    "available_metrics = [col for col in key_fundamental_metrics if col in fundamental_data.columns]\n",
    "fundamental_subset = fundamental_data[available_metrics].copy()\n",
    "\n",
    "print(f\" Selected fundamental metrics:\")\n",
    "for metric in available_metrics:\n",
    "    if metric != 'merge_date':\n",
    "        valid_count = fundamental_subset[metric].notna().sum()\n",
    "        print(f\"   - {metric}: {valid_count} valid values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bbf166",
   "metadata": {},
   "source": [
    "### 7: Final merge with fundamental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2fd5625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Final merge completed!\n",
      "   Final dataset shape: (104028, 22)\n",
      "   Total columns: 22\n",
      "   Records with fundamental data: 3595/104028 (3.5%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_data = pd.merge(\n",
    "    price_with_sentiment,\n",
    "    fundamental_subset,\n",
    "    on='merge_date',\n",
    "    how='left'  # Keep all price-sentiment records\n",
    ")\n",
    "\n",
    "print(f\"   Final merge completed!\")\n",
    "print(f\"   Final dataset shape: {merged_data.shape}\")\n",
    "print(f\"   Total columns: {len(merged_data.columns)}\")\n",
    "\n",
    "# Check fundamental data coverage\n",
    "if 'pe_op_basic' in merged_data.columns:\n",
    "    fundamental_coverage = merged_data['pe_op_basic'].notna().sum()\n",
    "    print(f\"   Records with fundamental data: {fundamental_coverage}/{len(merged_data)} ({fundamental_coverage/len(merged_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090c51f",
   "metadata": {},
   "source": [
    "### 8: Display final merged dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3f7625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL MERGED DATASET OVERVIEW\n",
      "============================================================\n",
      " Dataset Shape: (104028, 22)\n",
      " Date Range: 2018-11-30 to 2024-03-13\n",
      " Time Frequency: 5-minute intervals\n",
      " COLUMN CATEGORIES:\n",
      "   Price Data: 7 columns\n",
      "   Sentiment Data: 5 columns\n",
      "   Fundamental Data: 10 columns\n",
      "   DATA COMPLETENESS:\n",
      "   Total Records: 104,028\n",
      "   Unique Dates: 1376\n",
      "   Records with Sentiment: 74,220\n",
      "   Records with Fundamentals: 3,595\n",
      "   SAMPLE DATA (First 3 rows):\n",
      "             datetime  close  volume  daily_news_count  \\\n",
      "0 2018-11-30 17:50:00  44.63  179494               1.0   \n",
      "1 2018-11-30 17:55:00  44.52  218881               1.0   \n",
      "2 2018-11-30 18:00:00  44.49  364114               1.0   \n",
      "\n",
      "   avg_sentiment_polarity   CAPEI  \n",
      "0                   0.998  17.191  \n",
      "1                   0.998  17.191  \n",
      "2                   0.998  17.191  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL MERGED DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\" Dataset Shape: {merged_data.shape}\")\n",
    "print(f\" Date Range: {merged_data['merge_date'].min()} to {merged_data['merge_date'].max()}\")\n",
    "print(f\" Time Frequency: 5-minute intervals\")\n",
    "\n",
    "print(f\" COLUMN CATEGORIES:\")\n",
    "price_cols = [col for col in merged_data.columns if col in ['datetime', 'open', 'high', 'low', 'close', 'volume', 'merge_date']]\n",
    "sentiment_cols = [col for col in merged_data.columns if 'sentiment' in col.lower() or 'news' in col.lower()]\n",
    "fundamental_cols = [col for col in merged_data.columns if col in available_metrics and col != 'merge_date']\n",
    "\n",
    "print(f\"   Price Data: {len(price_cols)} columns\")\n",
    "print(f\"   Sentiment Data: {len(sentiment_cols)} columns\") \n",
    "print(f\"   Fundamental Data: {len(fundamental_cols)} columns\")\n",
    "\n",
    "print(f\"   DATA COMPLETENESS:\")\n",
    "print(f\"   Total Records: {len(merged_data):,}\")\n",
    "print(f\"   Unique Dates: {merged_data['merge_date'].nunique()}\")\n",
    "print(f\"   Records with Sentiment: {merged_data['avg_sentiment_polarity'].notna().sum():,}\")\n",
    "print(f\"   Records with Fundamentals: {merged_data[fundamental_cols[0]].notna().sum() if fundamental_cols else 0:,}\")\n",
    "\n",
    "print(f\"   SAMPLE DATA (First 3 rows):\")\n",
    "sample_cols = ['datetime', 'close', 'volume', 'daily_news_count', 'avg_sentiment_polarity']\n",
    "if fundamental_cols:\n",
    "    sample_cols.append(fundamental_cols[0])\n",
    "    \n",
    "print(merged_data[sample_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498fd9ed",
   "metadata": {},
   "source": [
    "### 9: Save the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9d964ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merged data saved to: ../data/processed/merged_data.pkl\n",
      " File size: 17.96 MB\n",
      "  DATA MERGING PROCESS COMPLETED SUCCESSFULLY!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_path = \"../data/processed/merged_data.pkl\"\n",
    "merged_data.to_pickle(output_path)\n",
    "\n",
    "print(f\" Merged data saved to: {output_path}\")\n",
    "print(f\" File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(\"  DATA MERGING PROCESS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf9f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 104028 entries, 0 to 104027\n",
      "Data columns (total 22 columns):\n",
      " #   Column                  Non-Null Count   Dtype         \n",
      "---  ------                  --------------   -----         \n",
      " 0   datetime                104028 non-null  datetime64[ns]\n",
      " 1   open                    104028 non-null  float64       \n",
      " 2   high                    104028 non-null  float64       \n",
      " 3   low                     104028 non-null  float64       \n",
      " 4   close                   104028 non-null  float64       \n",
      " 5   volume                  104028 non-null  int64         \n",
      " 6   merge_date              104028 non-null  object        \n",
      " 7   avg_sentiment_polarity  74220 non-null   float64       \n",
      " 8   avg_sentiment_neg       74220 non-null   float64       \n",
      " 9   avg_sentiment_neu       74220 non-null   float64       \n",
      " 10  avg_sentiment_pos       74220 non-null   float64       \n",
      " 11  daily_news_count        74220 non-null   float64       \n",
      " 12  CAPEI                   3595 non-null    float64       \n",
      " 13  bm                      3595 non-null    float64       \n",
      " 14  evm                     3595 non-null    float64       \n",
      " 15  pe_op_basic             3595 non-null    float64       \n",
      " 16  pe_op_dil               3595 non-null    float64       \n",
      " 17  roa                     3595 non-null    float64       \n",
      " 18  roe                     3595 non-null    float64       \n",
      " 19  npm                     3595 non-null    float64       \n",
      " 20  ps                      3595 non-null    float64       \n",
      " 21  de_ratio                3595 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(19), int64(1), object(1)\n",
      "memory usage: 17.5+ MB\n",
      "None\n",
      "             datetime   open   high    low  close  volume  merge_date  \\\n",
      "0 2018-11-30 17:50:00  44.60  44.64  44.55  44.63  179494  2018-11-30   \n",
      "1 2018-11-30 17:55:00  44.63  44.63  44.52  44.52  218881  2018-11-30   \n",
      "2 2018-11-30 18:00:00  44.52  44.55  44.44  44.49  364114  2018-11-30   \n",
      "3 2018-11-30 18:05:00  44.49  44.52  44.44  44.46  251652  2018-11-30   \n",
      "4 2018-11-30 18:10:00  44.45  44.51  44.44  44.45  200756  2018-11-30   \n",
      "\n",
      "   avg_sentiment_polarity  avg_sentiment_neg  avg_sentiment_neu  ...   CAPEI  \\\n",
      "0                   0.998              0.009              0.925  ...  17.191   \n",
      "1                   0.998              0.009              0.925  ...  17.191   \n",
      "2                   0.998              0.009              0.925  ...  17.191   \n",
      "3                   0.998              0.009              0.925  ...  17.191   \n",
      "4                   0.998              0.009              0.925  ...  17.191   \n",
      "\n",
      "    bm     evm  pe_op_basic  pe_op_dil    roa    roe    npm     ps  de_ratio  \n",
      "0  0.1  14.855       14.542     14.674  0.216  0.436  0.224  3.194     2.413  \n",
      "1  0.1  14.855       14.542     14.674  0.216  0.436  0.224  3.194     2.413  \n",
      "2  0.1  14.855       14.542     14.674  0.216  0.436  0.224  3.194     2.413  \n",
      "3  0.1  14.855       14.542     14.674  0.216  0.436  0.224  3.194     2.413  \n",
      "4  0.1  14.855       14.542     14.674  0.216  0.436  0.224  3.194     2.413  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Quick inspection of the saved file\n",
    "import pandas as pd\n",
    "data = pd.read_pickle(\"../data/processed/merged_data.pkl\")\n",
    "print(data.info())\n",
    "print(data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
